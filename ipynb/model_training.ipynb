{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULE IMPORTS\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG = dict(\n",
    "    NAME = \"premier_league\",\n",
    "    VERSION = \"v1.0\",\n",
    "    TYPE = \"win_outcome\"\n",
    ")\n",
    "load_dotenv(\"../boto3_cloudflare.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SHAPE: (1689, 11), Index(['day', 'month', 'year', 'weekday', 'hour', 'minute', 'matchday',\n",
      "       'homeId', 'awayId', 'scoreHomeHt', 'scoreAwayHt'],\n",
      "      dtype='object')\n",
      "TESTING SHAPE: (423, 11)\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATASET\n",
    "X = pd.read_csv(f\"https://raw.githubusercontent.com/lebyanelm/neural-trained-models/main/relative_datasets/cleaned/{MODEL_CONFIG['NAME']}-{MODEL_CONFIG['VERSION']}-train-set.csv\").drop_duplicates().dropna()\n",
    "Y = X.pop(\"target\")\n",
    "print(f\"\"\"TRAINING SHAPE: {X.shape}, {X.columns}\"\"\")\n",
    "\n",
    "# TESTING/EVALUATION DATASET\n",
    "X_test = pd.read_csv(f\"https://raw.githubusercontent.com/lebyanelm/neural-trained-models/main/relative_datasets/cleaned/{MODEL_CONFIG['NAME']}-{MODEL_CONFIG['VERSION']}-test-set.csv\").drop_duplicates().dropna()\n",
    "Y_test = X_test.pop(\"target\")\n",
    "print(f\"\"\"TESTING SHAPE: {X_test.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_as_sklearn(selected_model, best_transformer = None):\n",
    "    model_name = f\"{MODEL_CONFIG['NAME']}-{MODEL_CONFIG['VERSION']}-{MODEL_CONFIG['TYPE']}.joblib\"\n",
    "    transformer_name = f\"{MODEL_CONFIG['NAME']}-{MODEL_CONFIG['VERSION']}-{MODEL_CONFIG['TYPE']}-transformer.joblib\"\n",
    "    model_folder = f\"../models/{model_name}\"\n",
    "    transformer_folder = f\"../models/{transformer_name}\"\n",
    "    \n",
    "    \"\"\"Save te best selected model and the transformer used in training.\"\"\"\n",
    "    joblib.dump(selected_model, model_folder)\n",
    "    joblib.dump(best_transformer, transformer_folder)\n",
    "\n",
    "    if MODELS_REGISTRY[\"models\"].get(MODEL_CONFIG[\"NAME\"]) == None:\n",
    "        MODELS_REGISTRY[\"models\"][MODEL_CONFIG[\"NAME\"]] = dict()\n",
    "    \n",
    "    print(MODELS_REGISTRY)\n",
    "    MODELS_REGISTRY[\"models\"][MODEL_CONFIG[\"NAME\"]][MODEL_CONFIG[\"TYPE\"]] = dict(\n",
    "        model = model_name,\n",
    "        transformer = transformer_name,\n",
    "        params = selected_model.get_params()\n",
    "    )\n",
    "\n",
    "    print(MODELS_REGISTRY)\n",
    "    return save_model_registry()\n",
    "\n",
    "\n",
    "def load_model_registry():\n",
    "  with open(\"../models/models-registry.json\", \"r\") as registry:\n",
    "    model_registry_data = \"\".join(registry.readlines())\n",
    "    return json.loads(model_registry_data)\n",
    "MODELS_REGISTRY = load_model_registry()\n",
    "\n",
    "\n",
    "def save_model_registry(model_params = None):\n",
    "    with open(\"../models/models-registry.json\", \"wb\") as registry:\n",
    "      file_contents = json.dumps(MODELS_REGISTRY, indent=4)\n",
    "      registry.write(file_contents.encode())\n",
    "      print(\"Updated registry:\", MODELS_REGISTRY)\n",
    "\n",
    "\n",
    "def betslip_win_rate(X_test, y_test, model, odds_range, bet_amount = 10, matches_per_bet = 5):\n",
    "    \"\"\"\n",
    "    Calculate win and loss rate, and estimate winnings.\n",
    "\n",
    "    Parameters:\n",
    "    X_test (ndarray): Test features.\n",
    "    y_test (ndarray): Actual outcomes for the test set.\n",
    "    model (tf.keras.Model / sklearn Model): Trained prediction model.\n",
    "    odds (list): List of odds for each match.\n",
    "    bet_amount (float): Amount of money placed on each bet. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Average win rate, average loss rate, number of bets, total winnings.\n",
    "    \"\"\"\n",
    "    # Predict outcomes\n",
    "    y_preds = (model.predict(X_test).reshape(-1) > 0.5).astype(int)\n",
    "    odds = [round(random.random() * odds_range[-1], 2) for _ in range(0, len(y_preds))]\n",
    "    \n",
    "    # Ensure the lengths of the predictions and actual values are the same\n",
    "    if len(y_preds) != len(y_test) or len(y_preds) != len(odds):\n",
    "        raise ValueError(\"Predictions, actual values, and odds must have the same length\")\n",
    "\n",
    "    # Initialize counters for win, loss rates and bets\n",
    "    win_rate = 0\n",
    "    loss_rate = 0\n",
    "    bets_count = 0\n",
    "    total_winnings = 0\n",
    "    total_odds = 0\n",
    "    \n",
    "    # Loop over the predictions in chunks of matches_per_bet\n",
    "    for i in range(0, len(y_preds), matches_per_bet):\n",
    "        print(f\"Matches: {i}-{i+matches_per_bet}\")\n",
    "        win_count = 0\n",
    "        loss_count = 0\n",
    "        bet_winnings = 0\n",
    "        \n",
    "        # Ensure there are enough matches left for a full bet\n",
    "        if i + matches_per_bet > len(y_preds):\n",
    "            break\n",
    "\n",
    "        # Calculate wins and losses within the current bet and compute winnings\n",
    "        for j in range(i, i + matches_per_bet):\n",
    "            if y_preds[j] == y_test[j]:\n",
    "                win_count += 1\n",
    "                bet_winnings += odds[j] * bet_amount  # Calculate winnings for the match\n",
    "                total_odds += odds[j]\n",
    "                print(f\"W: R{round(bet_winnings, 2)}, (win count:  {win_count}, odds: {odds[j]}, bet amount: {bet_amount})\")\n",
    "            else:\n",
    "                loss_count += 1\n",
    "                bet_winnings -= bet_amount  # Subtract the bet amount for a loss\n",
    "                print(f\"L: R{round(bet_winnings, 2)}, (loss count:  {loss_count}, odds: {odds[j]}, bet amount: {bet_amount}))\")\n",
    "\n",
    "        # Update overall win and loss rates and bets count\n",
    "        if win_count + loss_count > 0:  # Avoid division by zero\n",
    "            win_rate += win_count / (win_count + loss_count)\n",
    "            loss_rate += loss_count / (win_count + loss_count)\n",
    "        bets_count += 1\n",
    "\n",
    "        total_winnings += bet_winnings - bet_amount\n",
    "\n",
    "        print(f\"Balance: R{total_winnings}\")\n",
    "        print(\"__________\")\n",
    "        print(\"\")\n",
    "\n",
    "    # Calculate average win and loss rates\n",
    "    if bets_count > 0:  # Avoid division by zero\n",
    "        avg_win_rate = win_rate / bets_count\n",
    "        avg_loss_rate = loss_rate / bets_count\n",
    "    else:\n",
    "        avg_win_rate = 0\n",
    "        avg_loss_rate = 0\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Win rate: {avg_win_rate:.2f}%\")\n",
    "    print(f\"Loss rate: {avg_loss_rate:.2f}%\")\n",
    "    print(f\"Bets count: {bets_count} bets\")\n",
    "    print(f\"Bets cost: R{round(bets_count * bet_amount, 2)}\")\n",
    "    print(f\"Total odds: x{round(total_odds, 2)}\")\n",
    "    print(f\"Total winnings: R{total_winnings:.2f}\")\n",
    "\n",
    "    return avg_win_rate, avg_loss_rate, bets_count, total_winnings\n",
    "\n",
    "\n",
    "def perfomance_metric(X_test, Y_test, model):\n",
    "  Y_preds = (model.predict(X_test).reshape(-1) > 0.5).astype(int)\n",
    "  combined_actual_to_preds = pd.DataFrame(dict(actual=Y_test, prediction=Y_preds))\n",
    "  print(classification_report(Y_test, Y_preds))\n",
    "  return pd.crosstab(index=combined_actual_to_preds[\"actual\"], columns=combined_actual_to_preds[\"prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Over Sampler:  (1964, 11)\n"
     ]
    }
   ],
   "source": [
    "# Randomly oversample the data to equalize the buy/sell count\n",
    "ros = RandomOverSampler()\n",
    "X_sampled, Y_sampled = ros.fit_resample(X.values, Y.values)\n",
    "print(\"Random Over Sampler: \", X_sampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA NORMALIZATION AND STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8       , 0.90909091, 0.66666667, ..., 0.26641998, 0.2       ,\n",
       "        0.25      ],\n",
       "       [0.3       , 1.        , 0.33333333, ..., 0.01017576, 0.        ,\n",
       "        0.25      ],\n",
       "       [0.33333333, 0.90909091, 0.66666667, ..., 0.2506938 , 0.2       ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.13333333, 0.        , 0.66666667, ..., 0.00740056, 0.        ,\n",
       "        0.        ],\n",
       "       [0.36666667, 0.63636364, 0.66666667, ..., 0.01110083, 0.        ,\n",
       "        0.25      ],\n",
       "       [0.23333333, 0.36363636, 0.66666667, ..., 0.01295097, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MIN_MAX SCALER\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_sampled)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD SCALER\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_sampled)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.625     ,  0.42857143,  0.        , ...,  0.0221519 ,\n",
       "         1.        ,  1.        ],\n",
       "       [-0.3125    ,  0.57142857, -1.        , ..., -0.85443038,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.25      ,  0.42857143,  0.        , ..., -0.03164557,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.875     ,  0.42857143, -1.        , ...,  0.01582278,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.625     ,  0.42857143, -1.        , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.625     ,  0.42857143, -1.        , ..., -0.85443038,\n",
       "         0.        ,  1.        ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROBUST SCALER\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X_sampled)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX ABS SCALER\n",
    "scaler = MaxAbsScaler()\n",
    "X_scaled = scaler.fit_transform(X_sampled)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LogisticRegression: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best cross-validation accuracy for LogisticRegression: 0.7566183725398556\n",
      "Running GridSearchCV for RandomForestClassifier...\n",
      "Best parameters for RandomForestClassifier: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best cross-validation accuracy for RandomForestClassifier: 0.8233512488965052\n",
      "Running GridSearchCV for SVC...\n",
      "Best parameters for SVC: {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "Best cross-validation accuracy for SVC: 0.774462533104845\n",
      "Running GridSearchCV for KNeighborsClassifier...\n",
      "Best parameters for KNeighborsClassifier: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}\n",
      "Best cross-validation accuracy for KNeighborsClassifier: 0.7693929480189022\n",
      "Running GridSearchCV for DecisionTreeClassifier...\n",
      "Best parameters for DecisionTreeClassifier: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 2}\n",
      "Best cross-validation accuracy for DecisionTreeClassifier: 0.7861868411486733\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001],\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "best_params = {}\n",
    "for model_name in models:\n",
    "    print(f\"Running GridSearchCV for {model_name}...\")\n",
    "    grid_search = GridSearchCV(estimator=models[model_name], param_grid=param_grids[model_name], cv=5, n_jobs=-1, scoring='accuracy')\n",
    "    grid_search.fit(X_scaled, Y_sampled)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    best_params[model_name] = grid_search.best_params_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation accuracy for {model_name}: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# SAVE THE BEST SELECTED MODEL:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msave_model_as_sklearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 9\u001b[0m, in \u001b[0;36msave_model_as_sklearn\u001b[0;34m(selected_model)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m MODELS_REGISTRY[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(MODEL_CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAME\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     MODELS_REGISTRY[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m][MODEL_CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAME\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m----> 9\u001b[0m \u001b[43mMODELS_REGISTRY\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMODEL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNAME\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMODEL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTYPE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     10\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_name,\n\u001b[1;32m     11\u001b[0m     params \u001b[38;5;241m=\u001b[39m selected_model\u001b[38;5;241m.\u001b[39mget_params()\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(MODELS_REGISTRY)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# SAVE THE BEST SELECTED MODEL:\n",
    "save_model_as_sklearn(selected_model=best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_model_params = None\n",
    "precision_benchmark = 0\n",
    "for model_name, model in best_models.items():\n",
    "    y_pred = model.predict(scaler.transform(X_test.values))\n",
    "    precision = precision_score(Y_test, y_pred)\n",
    "    if precision > precision_benchmark:\n",
    "        precision_benchmark = precision\n",
    "        best_model = model\n",
    "        best_model_params = best_params[model_name]\n",
    "        \n",
    "    print(f\"Test precision_score for {model_name}: {precision:.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"--------------------\")\n",
    "print()\n",
    "\n",
    "# BETSLIP EVALUATIONS\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "betslip_win_rate(X_scaled, Y_sampled, odds_range=[1, 3], model=best_model, bet_amount=10, matches_per_bet=1)\n",
    "perfomance_metric(X_scaled, Y_sampled, model=best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_as_sklearn(selected_model=best_model, best_transformer=scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
