{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULE IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import joblib\n",
    "import locale\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG = dict(\n",
    "    NAME = \"fulltime_win_outcome\",\n",
    "    VERSION = \"v1.0\"\n",
    ")\n",
    "load_dotenv(\"../boto3_cloudflare.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SHAPE: (11551, 6), Index(['Month', 'Day', 'Time', 'AvgH', 'AvgD', 'AvgA'], dtype='object')\n",
      "TESTING SHAPE: (2881, 6)\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATASET\n",
    "X = pd.read_csv(f\"https://raw.githubusercontent.com/lebyanelm/neural-trained-models/main/relative_datasets/cleaned/{MODEL_CONFIG['NAME']}_{MODEL_CONFIG['VERSION']}-train-set.csv\").drop_duplicates().dropna().reset_index(drop=True)\n",
    "Y = X.pop(\"target\")\n",
    "print(f\"\"\"TRAINING SHAPE: {X.shape}, {X.columns}\"\"\")\n",
    "\n",
    "# TESTING/EVALUATION DATASET\n",
    "X_eval = pd.read_csv(f\"https://raw.githubusercontent.com/lebyanelm/neural-trained-models/main/relative_datasets/cleaned/{MODEL_CONFIG['NAME']}_{MODEL_CONFIG['VERSION']}-test-set.csv\").drop_duplicates().dropna().reset_index(drop=True)\n",
    "Y_test = X_eval.pop(\"target\")\n",
    "print(f\"\"\"TESTING SHAPE: {X_eval.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_as_sklearn(selected_model, best_transformer = None):\n",
    "    model_name = f\"{MODEL_CONFIG['NAME']}-{MODEL_CONFIG['VERSION']}-{MODEL_CONFIG['TYPE']}.joblib\"\n",
    "    transformer_name = f\"{MODEL_CONFIG['NAME']}-{MODEL_CONFIG['VERSION']}-{MODEL_CONFIG['TYPE']}-transformer.joblib\"\n",
    "    model_folder = f\"../models/{model_name}\"\n",
    "    transformer_folder = f\"../models/{transformer_name}\"\n",
    "    \n",
    "    \"\"\"Save te best selected model and the transformer used in training.\"\"\"\n",
    "    joblib.dump(selected_model, model_folder)\n",
    "    joblib.dump(best_transformer, transformer_folder)\n",
    "\n",
    "    MODELS_REGISTRY[\"models\"][MODEL_CONFIG[\"NAME\"]] = dict(\n",
    "        model = model_name,\n",
    "        transformer = transformer_name,\n",
    "        params = selected_model.get_params()\n",
    "    )\n",
    "    print(MODELS_REGISTRY)\n",
    "    return save_model_registry()\n",
    "\n",
    "\n",
    "def load_model_registry():\n",
    "  with open(\"../models/models-registry.json\", \"r\") as registry:\n",
    "    model_registry_data = \"\".join(registry.readlines())\n",
    "    return json.loads(model_registry_data)\n",
    "MODELS_REGISTRY = load_model_registry()\n",
    "\n",
    "\n",
    "def save_model_registry(model_params = None):\n",
    "    with open(\"../models/models-registry.json\", \"wb\") as registry:\n",
    "      file_contents = json.dumps(MODELS_REGISTRY, indent=4)\n",
    "      registry.write(file_contents.encode())\n",
    "      print(\"Updated registry:\", MODELS_REGISTRY)\n",
    "\n",
    "\n",
    "def format_number_with_space(number):\n",
    "    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "    formatted_number = locale.format_string(\"%n\", number, grouping=True).replace(',', ' ')\n",
    "    return formatted_number\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def betslip_win_rate(X_eval, y_eval, model, odds, bet_amount=10, matches_per_bet=5, broker_balance=100, available_topup=100, log=False):\n",
    "    \"\"\"\n",
    "    Calculate win and loss rate, and estimate winnings.\n",
    "\n",
    "    Parameters:\n",
    "    X_test (ndarray): Test features.\n",
    "    y_test (ndarray): Actual outcomes for the test set.\n",
    "    model (tf.keras.Model / sklearn Model): Trained prediction model.\n",
    "    odds (list): List of odds for each match.\n",
    "    bet_amount (float): Amount of money placed on each bet. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Average win rate, average loss rate, total odds, final broker balance.\n",
    "    \"\"\"\n",
    "    # Generate predictions\n",
    "    if hasattr(model, 'predict'):\n",
    "        predictions = model.predict(X_eval)\n",
    "    else:\n",
    "        raise ValueError(\"Model does not have a predict method.\")\n",
    "    \n",
    "    # Ensure predictions are binary (0 or 1)\n",
    "    predictions = (predictions > 0.5).astype(bool)\n",
    "    \n",
    "    # Initialize counts and earnings\n",
    "    win_count = 0\n",
    "    loss_count = 0\n",
    "    odds_earned = 0\n",
    "\n",
    "    # Loop over matches per bet\n",
    "    for i in range(0, len(y_eval), matches_per_bet):\n",
    "        # Calculate the stake as 10% of the available balance\n",
    "        current_bet_amount = round(broker_balance * 0.1, 2)\n",
    "        broker_balance -= current_bet_amount\n",
    "        \n",
    "        # If balance is too low to continue betting, break the loop\n",
    "        if broker_balance < 0:\n",
    "            logging.info(\"Balance depleted.\")\n",
    "            break\n",
    "        \n",
    "        # Calculate total odds for the current set of matches\n",
    "        total_odds = math.prod(odds[i:i+matches_per_bet])\n",
    "        if log:\n",
    "            print(\"---------------------------------\", i)\n",
    "            print(f\"{X_test.iloc[i].day}, {X_test.iloc[i].month}, {X_test.iloc[i].year}: bet amount           | R{current_bet_amount:.2f} ({total_odds})\".replace(\",\", \" \"))\n",
    "            print(f\"{X_test.iloc[i].day}, {X_test.iloc[i].month}, {X_test.iloc[i].year}: balance              | R{broker_balance:.2f}\".replace(\",\", \" \"))\n",
    "        \n",
    "        # Determine if the current bet is a win or loss\n",
    "        betslip_comparison = predictions[i:i+matches_per_bet] == y_eval[i:i+matches_per_bet]\n",
    "        if all(betslip_comparison):\n",
    "            broker_balance += current_bet_amount * total_odds\n",
    "            if log:\n",
    "                print(f\"{X_test.iloc[i].day} {X_test.iloc[i].month} {X_test.iloc[i].year}: amount earned           | R{current_bet_amount*total_odds:.2f}\".replace(\",\", \" \"))\n",
    "                print(f\"{X_test.iloc[i].day}, {X_test.iloc[i].month}, {X_test.iloc[i].year}: balance               | R{broker_balance:.2f}\".replace(\",\", \" \"))\n",
    "                print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "            odds_earned += total_odds\n",
    "            win_count += 1\n",
    "        else:\n",
    "            loss_count += 1\n",
    "        \n",
    "        # If balance drops below zero, consider the available topup\n",
    "        if broker_balance < 0 and available_topup > 0:\n",
    "            topup_needed = -broker_balance\n",
    "            topup_used = min(topup_needed, available_topup)\n",
    "            broker_balance += topup_used\n",
    "            available_topup -= topup_used\n",
    "            if broker_balance < 0:\n",
    "                broker_balance = 0  # Ensure the balance does not go negative\n",
    "\n",
    "\n",
    "    # Calculate average win rate and loss rate\n",
    "    total_bets = win_count + loss_count\n",
    "    avg_win_rate = win_count / total_bets if total_bets else 0\n",
    "    avg_loss_rate = loss_count / total_bets if total_bets else 0\n",
    "        \n",
    "    logging.info(f\"win rate           | {avg_win_rate*100:.2f}%\")\n",
    "    logging.info(f\"loss rate          | {avg_loss_rate*100:.2f}%\")\n",
    "    logging.info(f\"odds earned        | {odds_earned:.2f}\")\n",
    "    logging.info(f\"final balance      | R{broker_balance:,.2f}\".replace(\",\", \" \"))\n",
    "    logging.info(f\"topup left         | R{available_topup:,.2f}\".replace(\",\", \" \"))\n",
    "  \n",
    "    return avg_win_rate, avg_loss_rate, total_odds, broker_balance, available_topup\n",
    "\n",
    "# Example usage:\n",
    "# X_test, y_test should be numpy arrays, model should be a trained model, odds should be a list of odds\n",
    "# avg_win_rate, avg_loss_rate, total_odds, final_balance = betslip_win_rate(X_test, y_test, model, odds)\n",
    "\n",
    "\n",
    "def perfomance_metric(X_test, Y_test, model):\n",
    "    Y_preds = (model.predict(X_test).reshape(-1) > 0.5).astype(int)\n",
    "    combined_actual_to_preds = pd.DataFrame(dict(actual=Y_test, prediction=Y_preds))\n",
    "    print(classification_report(Y_test, Y_preds))\n",
    "    return pd.crosstab(index=combined_actual_to_preds[\"actual\"], columns=combined_actual_to_preds[\"prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Over Sampler:  (15252, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time</th>\n",
       "      <th>AvgH</th>\n",
       "      <th>AvgD</th>\n",
       "      <th>AvgA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>2.49</td>\n",
       "      <td>3.31</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>2.77</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2.38</td>\n",
       "      <td>3.51</td>\n",
       "      <td>2.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15247</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>5.60</td>\n",
       "      <td>4.23</td>\n",
       "      <td>1.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15248</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2.42</td>\n",
       "      <td>3.38</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15249</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>2.66</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15250</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>1.72</td>\n",
       "      <td>3.48</td>\n",
       "      <td>5.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15251</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1.71</td>\n",
       "      <td>3.88</td>\n",
       "      <td>4.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15252 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Month  Day  Time  AvgH  AvgD  AvgA\n",
       "0          7    4    19  2.49  3.31  2.80\n",
       "1          7    5    19  2.77  3.47  2.43\n",
       "2          7    5    12  2.38  3.51  2.81\n",
       "3          7    5    12  3.15  3.24  2.29\n",
       "4          7    5    12  3.55  3.84  1.93\n",
       "...      ...  ...   ...   ...   ...   ...\n",
       "15247      3    2    19  5.60  4.23  1.55\n",
       "15248      8    5    15  2.42  3.38  2.63\n",
       "15249      8    5    20  2.66  3.08  2.74\n",
       "15250     10    5    18  1.72  3.48  5.12\n",
       "15251     10    1    20  1.71  3.88  4.74\n",
       "\n",
       "[15252 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly oversample the data to equalize the buy/sell count\n",
    "ros = RandomOverSampler()\n",
    "X_resampled, Y_resampled = ros.fit_resample(X, Y)\n",
    "print(\"Random Over Sampler: \", X_resampled.shape)\n",
    "# X_resampled = X_resampled.sort_values([\"year\", \"month\", \"day\", \"matchday\", \"hour\", \"minute\"])\n",
    "X_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA NORMALIZATION AND STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54545455, 0.66666667, 0.75      , 0.0399339 , 0.10141844,\n",
       "        0.04790257],\n",
       "       [0.54545455, 0.83333333, 0.75      , 0.04764528, 0.11276596,\n",
       "        0.03788904],\n",
       "       [0.54545455, 0.83333333, 0.16666667, 0.03690443, 0.11560284,\n",
       "        0.04817321],\n",
       "       ...,\n",
       "       [0.63636364, 0.83333333, 0.83333333, 0.04461581, 0.08510638,\n",
       "        0.04627876],\n",
       "       [0.81818182, 0.83333333, 0.66666667, 0.01872762, 0.11347518,\n",
       "        0.11069012],\n",
       "       [0.81818182, 0.16666667, 0.83333333, 0.01845222, 0.14184397,\n",
       "        0.10040595]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MIN_MAX SCALER\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled.values)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD SCALER\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled.values)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROBUST SCALER\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled.values)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX ABS SCALER\n",
    "scaler = MaxAbsScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best parameters for LogisticRegression: {'classifier__C': 1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "Best cross-validation score for LogisticRegression: 0.5110388673781144\n",
      "\n",
      "Running GridSearchCV for RandomForestClassifier\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 67\u001b[0m\n\u001b[1;32m     57\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     58\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipeline,\n\u001b[1;32m     59\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grids[clf_name],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Fit the GridSearchCV\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Get the best model and its parameters\u001b[39;00m\n\u001b[1;32m     70\u001b[0m best_models[clf_name] \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(multi_class='auto'),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    # 'SVC': SVC(),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'classifier__penalty': ['l2'],\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20, 30],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'classifier__max_depth': [None, 10, 20, 30],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__criterion': ['gini', 'entropy']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a dictionary of classifiers\n",
    "classifiers = {\n",
    "    'LogisticRegression': LogisticRegression(multi_class='auto', max_iter=10000),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# Placeholder for best models\n",
    "best_models = {}\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "# X_train = ...\n",
    "# y_train = ...\n",
    "from sklearn.pipeline import Pipeline\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"Running GridSearchCV for {clf_name}\")\n",
    "    \n",
    "    # Create a pipeline with a scaler and the classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    \n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grids[clf_name],\n",
    "        scoring='accuracy',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the GridSearchCV\n",
    "    grid_search.fit(X, Y)\n",
    "    \n",
    "    # Get the best model and its parameters\n",
    "    best_models[clf_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {clf_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score for {clf_name}: {grid_search.best_score_}\\n\")\n",
    "\n",
    "# best_models = {}\n",
    "# best_params = {}\n",
    "# for model_name in models:\n",
    "#     print(f\"Running GridSearchCV for {model_name}...\")\n",
    "#     grid_search = GridSearchCV(estimator=models[model_name], param_grid=param_grids[model_name], cv=5, n_jobs=-1, scoring='precision')\n",
    "#     grid_search.fit(X_scaled, Y_resampled)\n",
    "#     best_models[model_name] = grid_search.best_estimator_\n",
    "#     best_params[model_name] = grid_search.best_params_\n",
    "#     print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "#     print(f\"Best cross-validation accuracy for {model_name}: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "odds = [round(1 + random.random() * 1.5, 2) for _ in range(0, len(X_test_scaled))]\n",
    "\n",
    "best_model = None\n",
    "precision_benchmark = 0\n",
    "for model_name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    precision = precision_score(Y_test, y_pred)\n",
    "    if precision > precision_benchmark:\n",
    "        precision_benchmark = precision\n",
    "        best_model = model\n",
    "    print(f\"Test precision_score for {model_name}: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "\n",
    "# BETSLIP EVALUATIONS\n",
    "betslip_win_rate(X_test_scaled, Y_test, model=best_model, odds=odds, matches_per_bet=1, broker_balance = 100, log=True)\n",
    "\n",
    "print()\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "\n",
    "perfomance_metric(X_test_scaled, Y_test, model=best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE BEST SELECTED MODEL:\n",
    "save_model_as_sklearn(selected_model=best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root': 'https://raw.githubusercontent.com/lebyanelm/neural-trained-models/main/models/', 'models': {'premier_league': {'win_outcome': {'model': 'premier_league-v1.0-win_outcome.joblib', 'transformer': 'premier_league-v1.0-win_outcome-transformer.joblib', 'params': {'C': 0.1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}}, 'all_leagues': {}}}\n",
      "{'root': 'https://raw.githubusercontent.com/lebyanelm/neural-trained-models/main/models/', 'models': {'premier_league': {'win_outcome': {'model': 'premier_league-v1.0-win_outcome.joblib', 'transformer': 'premier_league-v1.0-win_outcome-transformer.joblib', 'params': {'C': 0.1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}}, 'all_leagues': {'fulltime_win_outcome': {'model': 'all_leagues-v1.0-fulltime_win_outcome.joblib', 'transformer': 'all_leagues-v1.0-fulltime_win_outcome-transformer.joblib', 'params': {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}}}}\n",
      "Updated registry: {'root': 'https://raw.githubusercontent.com/lebyanelm/neural-trained-models/main/models/', 'models': {'premier_league': {'win_outcome': {'model': 'premier_league-v1.0-win_outcome.joblib', 'transformer': 'premier_league-v1.0-win_outcome-transformer.joblib', 'params': {'C': 0.1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}}, 'all_leagues': {'fulltime_win_outcome': {'model': 'all_leagues-v1.0-fulltime_win_outcome.joblib', 'transformer': 'all_leagues-v1.0-fulltime_win_outcome-transformer.joblib', 'params': {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}}}}\n"
     ]
    }
   ],
   "source": [
    "save_model_as_sklearn(selected_model=best_model, best_transformer=scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
