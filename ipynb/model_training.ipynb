{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULE IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import joblib\n",
    "import locale\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = dict(\n",
    "    NAME = \"all_leagues\",\n",
    "    VERSION = \"v1.0\",\n",
    "    TYPE = \"fulltime_win_outcome\"\n",
    ")\n",
    "load_dotenv(\"../boto3_cloudflare.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING DATASET\n",
    "X = pd.read_csv(f\"https://raw.githubusercontent.com/lebyanelm/neural-trained-models/main/relative_datasets/cleaned/{MODEL_CONFIG['NAME']}_{MODEL_CONFIG['TYPE']}-{MODEL_CONFIG['VERSION']}-train-set.csv\").drop_duplicates().dropna()\n",
    "X = X.sort_values([\"year\", \"month\", \"day\", \"weekday\", \"matchday\", \"hour\"])\n",
    "Y = X.pop(\"target\")\n",
    "print(f\"\"\"TRAINING SHAPE: {X.shape}, {X.columns}\"\"\")\n",
    "\n",
    "# TESTING/EVALUATION DATASET\n",
    "X_eval = pd.read_csv(f\"https://raw.githubusercontent.com/lebyanelm/neural-trained-models/main/relative_datasets/cleaned/{MODEL_CONFIG['NAME']}_{MODEL_CONFIG['TYPE']}-{MODEL_CONFIG['VERSION']}-test-set.csv\").drop_duplicates().dropna()\n",
    "X_eval = X_eval.sort_values([\"year\", \"month\", \"day\", \"weekday\", \"matchday\", \"hour\"])\n",
    "Y_test = X_eval.pop(\"target\")\n",
    "print(f\"\"\"TESTING SHAPE: {X_eval.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_as_sklearn(selected_model, best_transformer = None):\n",
    "    model_name = f\"{MODEL_CONFIG['NAME']}-{MODEL_CONFIG['VERSION']}-{MODEL_CONFIG['TYPE']}.joblib\"\n",
    "    transformer_name = f\"{MODEL_CONFIG['NAME']}-{MODEL_CONFIG['VERSION']}-{MODEL_CONFIG['TYPE']}-transformer.joblib\"\n",
    "    model_folder = f\"../models/{model_name}\"\n",
    "    transformer_folder = f\"../models/{transformer_name}\"\n",
    "    \n",
    "    \"\"\"Save te best selected model and the transformer used in training.\"\"\"\n",
    "    joblib.dump(selected_model, model_folder)\n",
    "    joblib.dump(best_transformer, transformer_folder)\n",
    "\n",
    "    if MODELS_REGISTRY[\"models\"].get(MODEL_CONFIG[\"NAME\"]) == None:\n",
    "        MODELS_REGISTRY[\"models\"][MODEL_CONFIG[\"NAME\"]] = dict()\n",
    "    \n",
    "    print(MODELS_REGISTRY)\n",
    "    MODELS_REGISTRY[\"models\"][MODEL_CONFIG[\"NAME\"]][MODEL_CONFIG[\"TYPE\"]] = dict(\n",
    "        model = model_name,\n",
    "        transformer = transformer_name,\n",
    "        params = selected_model.get_params()\n",
    "    )\n",
    "\n",
    "    print(MODELS_REGISTRY)\n",
    "    return save_model_registry()\n",
    "\n",
    "\n",
    "def load_model_registry():\n",
    "  with open(\"../models/models-registry.json\", \"r\") as registry:\n",
    "    model_registry_data = \"\".join(registry.readlines())\n",
    "    return json.loads(model_registry_data)\n",
    "MODELS_REGISTRY = load_model_registry()\n",
    "\n",
    "\n",
    "def save_model_registry(model_params = None):\n",
    "    with open(\"../models/models-registry.json\", \"wb\") as registry:\n",
    "      file_contents = json.dumps(MODELS_REGISTRY, indent=4)\n",
    "      registry.write(file_contents.encode())\n",
    "      print(\"Updated registry:\", MODELS_REGISTRY)\n",
    "\n",
    "\n",
    "def format_number_with_space(number):\n",
    "    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "    formatted_number = locale.format_string(\"%n\", number, grouping=True).replace(',', ' ')\n",
    "    return formatted_number\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def betslip_win_rate(X_eval, y_eval, model, odds, bet_amount=10, matches_per_bet=5, broker_balance=100, available_topup=100, log=False):\n",
    "    \"\"\"\n",
    "    Calculate win and loss rate, and estimate winnings.\n",
    "\n",
    "    Parameters:\n",
    "    X_test (ndarray): Test features.\n",
    "    y_test (ndarray): Actual outcomes for the test set.\n",
    "    model (tf.keras.Model / sklearn Model): Trained prediction model.\n",
    "    odds (list): List of odds for each match.\n",
    "    bet_amount (float): Amount of money placed on each bet. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Average win rate, average loss rate, total odds, final broker balance.\n",
    "    \"\"\"\n",
    "    # Generate predictions\n",
    "    if hasattr(model, 'predict'):\n",
    "        predictions = model.predict(X_eval)\n",
    "    else:\n",
    "        raise ValueError(\"Model does not have a predict method.\")\n",
    "    \n",
    "    # Ensure predictions are binary (0 or 1)\n",
    "    predictions = (predictions > 0.5).astype(bool)\n",
    "    \n",
    "    # Initialize counts and earnings\n",
    "    win_count = 0\n",
    "    loss_count = 0\n",
    "    odds_earned = 0\n",
    "\n",
    "    # Loop over matches per bet\n",
    "    for i in range(0, len(y_eval), matches_per_bet):\n",
    "        # Calculate the stake as 10% of the available balance\n",
    "        current_bet_amount = round(broker_balance * 0.1, 2)\n",
    "        broker_balance -= current_bet_amount\n",
    "        \n",
    "        # If balance is too low to continue betting, break the loop\n",
    "        if broker_balance < 0:\n",
    "            logging.info(\"Balance depleted.\")\n",
    "            break\n",
    "        \n",
    "        # Calculate total odds for the current set of matches\n",
    "        total_odds = math.prod(odds[i:i+matches_per_bet])\n",
    "        if log:\n",
    "            print(\"---------------------------------\", i)\n",
    "            print(f\"{X_test.iloc[i].day}, {X_test.iloc[i].month}, {X_test.iloc[i].year}: bet amount           | R{current_bet_amount:.2f} ({total_odds})\".replace(\",\", \" \"))\n",
    "            print(f\"{X_test.iloc[i].day}, {X_test.iloc[i].month}, {X_test.iloc[i].year}: balance              | R{broker_balance:.2f}\".replace(\",\", \" \"))\n",
    "        \n",
    "        # Determine if the current bet is a win or loss\n",
    "        betslip_comparison = predictions[i:i+matches_per_bet] == y_eval[i:i+matches_per_bet]\n",
    "        if all(betslip_comparison):\n",
    "            broker_balance += current_bet_amount * total_odds\n",
    "            if log:\n",
    "                print(f\"{X_test.iloc[i].day} {X_test.iloc[i].month} {X_test.iloc[i].year}: amount earned           | R{current_bet_amount*total_odds:.2f}\".replace(\",\", \" \"))\n",
    "                print(f\"{X_test.iloc[i].day}, {X_test.iloc[i].month}, {X_test.iloc[i].year}: balance               | R{broker_balance:.2f}\".replace(\",\", \" \"))\n",
    "                print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "            odds_earned += total_odds\n",
    "            win_count += 1\n",
    "        else:\n",
    "            loss_count += 1\n",
    "        \n",
    "        # If balance drops below zero, consider the available topup\n",
    "        if broker_balance < 0 and available_topup > 0:\n",
    "            topup_needed = -broker_balance\n",
    "            topup_used = min(topup_needed, available_topup)\n",
    "            broker_balance += topup_used\n",
    "            available_topup -= topup_used\n",
    "            if broker_balance < 0:\n",
    "                broker_balance = 0  # Ensure the balance does not go negative\n",
    "\n",
    "\n",
    "    # Calculate average win rate and loss rate\n",
    "    total_bets = win_count + loss_count\n",
    "    avg_win_rate = win_count / total_bets if total_bets else 0\n",
    "    avg_loss_rate = loss_count / total_bets if total_bets else 0\n",
    "        \n",
    "    logging.info(f\"win rate           | {avg_win_rate*100:.2f}%\")\n",
    "    logging.info(f\"loss rate          | {avg_loss_rate*100:.2f}%\")\n",
    "    logging.info(f\"odds earned        | {odds_earned:.2f}\")\n",
    "    logging.info(f\"final balance      | R{broker_balance:,.2f}\".replace(\",\", \" \"))\n",
    "    logging.info(f\"topup left         | R{available_topup:,.2f}\".replace(\",\", \" \"))\n",
    "  \n",
    "    return avg_win_rate, avg_loss_rate, total_odds, broker_balance, available_topup\n",
    "\n",
    "# Example usage:\n",
    "# X_test, y_test should be numpy arrays, model should be a trained model, odds should be a list of odds\n",
    "# avg_win_rate, avg_loss_rate, total_odds, final_balance = betslip_win_rate(X_test, y_test, model, odds)\n",
    "\n",
    "\n",
    "def perfomance_metric(X_test, Y_test, model):\n",
    "    Y_preds = (model.predict(X_test).reshape(-1) > 0.5).astype(int)\n",
    "    combined_actual_to_preds = pd.DataFrame(dict(actual=Y_test, prediction=Y_preds))\n",
    "    print(classification_report(Y_test, Y_preds))\n",
    "    return pd.crosstab(index=combined_actual_to_preds[\"actual\"], columns=combined_actual_to_preds[\"prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly oversample the data to equalize the buy/sell count\n",
    "ros = RandomOverSampler()\n",
    "X_resampled, Y_resampled = ros.fit_resample(X, Y)\n",
    "print(\"Random Over Sampler: \", X_resampled.shape)\n",
    "# X_resampled = X_resampled.sort_values([\"year\", \"month\", \"day\", \"matchday\", \"hour\", \"minute\"])\n",
    "X_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA NORMALIZATION AND STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN_MAX SCALER\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD SCALER\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROBUST SCALER\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX ABS SCALER\n",
    "scaler = MaxAbsScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    # 'SVC': SVC(),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    # 'SVC': {\n",
    "    #     'C': [0.1, 1, 10, 100],\n",
    "    #     'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    #     'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "    # },\n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "best_params = {}\n",
    "for model_name in models:\n",
    "    print(f\"Running GridSearchCV for {model_name}...\")\n",
    "    grid_search = GridSearchCV(estimator=models[model_name], param_grid=param_grids[model_name], cv=5, n_jobs=-1, scoring='precision')\n",
    "    grid_search.fit(X_scaled, Y_resampled)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    best_params[model_name] = grid_search.best_params_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation accuracy for {model_name}: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "odds = [round(1 + random.random() * 1.5, 2) for _ in range(0, len(X_test_scaled))]\n",
    "\n",
    "best_model = None\n",
    "precision_benchmark = 0\n",
    "for model_name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    precision = precision_score(Y_test, y_pred)\n",
    "    if precision > precision_benchmark:\n",
    "        precision_benchmark = precision\n",
    "        best_model = model\n",
    "    print(f\"Test precision_score for {model_name}: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "\n",
    "# BETSLIP EVALUATIONS\n",
    "betslip_win_rate(X_test_scaled, Y_test, model=best_model, odds=odds, matches_per_bet=1, broker_balance = 100, log=True)\n",
    "\n",
    "print()\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "\n",
    "perfomance_metric(X_test_scaled, Y_test, model=best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE BEST SELECTED MODEL:\n",
    "save_model_as_sklearn(selected_model=best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root': 'https://raw.githubusercontent.com/lebyanelm/neural-trained-models/main/models/', 'models': {'premier_league': {'win_outcome': {'model': 'premier_league-v1.0-win_outcome.joblib', 'transformer': 'premier_league-v1.0-win_outcome-transformer.joblib', 'params': {'C': 0.1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}}, 'all_leagues': {}}}\n",
      "{'root': 'https://raw.githubusercontent.com/lebyanelm/neural-trained-models/main/models/', 'models': {'premier_league': {'win_outcome': {'model': 'premier_league-v1.0-win_outcome.joblib', 'transformer': 'premier_league-v1.0-win_outcome-transformer.joblib', 'params': {'C': 0.1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}}, 'all_leagues': {'fulltime_win_outcome': {'model': 'all_leagues-v1.0-fulltime_win_outcome.joblib', 'transformer': 'all_leagues-v1.0-fulltime_win_outcome-transformer.joblib', 'params': {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}}}}\n",
      "Updated registry: {'root': 'https://raw.githubusercontent.com/lebyanelm/neural-trained-models/main/models/', 'models': {'premier_league': {'win_outcome': {'model': 'premier_league-v1.0-win_outcome.joblib', 'transformer': 'premier_league-v1.0-win_outcome-transformer.joblib', 'params': {'C': 0.1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}}, 'all_leagues': {'fulltime_win_outcome': {'model': 'all_leagues-v1.0-fulltime_win_outcome.joblib', 'transformer': 'all_leagues-v1.0-fulltime_win_outcome-transformer.joblib', 'params': {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}}}}\n"
     ]
    }
   ],
   "source": [
    "save_model_as_sklearn(selected_model=best_model, best_transformer=scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
